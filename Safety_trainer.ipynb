{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from lib import Helper_FCN as HFCN\n",
    "\n",
    "import tensorflow as tf\n",
    "import scipy.io\n",
    "\n",
    "from keras import layers\n",
    "from keras.optimizers import Adam, SGD\n",
    "from keras.callbacks import TensorBoard, TerminateOnNaN, EarlyStopping, ReduceLROnPlateau, ModelCheckpoint, LearningRateScheduler\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.utils import Sequence, plot_model\n",
    "from keras.losses import Loss\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "main_dir = 'Files/'\n",
    "total_data = 575263 Length of the dataset\n",
    "\n",
    "train_data = int(np.round((25600)/batch_size))\n",
    "\n",
    "print(\"Cada epoch ve \" + str(train_data) + \" batches de datos en entrenamiento, en total \" + str(train_data*batch_size) + \" datos y un \" + str(np.round(train_data*batch_size/total_data,2)) + \" porciento del total\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "separated_orbits = True #Keep as true to separate the orbits in the dataset, GUI and pretrained models are designed this way\n",
    "\n",
    "mean_extension = 2000 #Samples of points per file to padd or cut the data\n",
    "epochs = 500 #Number of epochs to train the model\n",
    "\n",
    "def process_data(test=False):\n",
    "    xVectors_batch = []\n",
    "    Ics_batch = []\n",
    "    Us_batch = []\n",
    "\n",
    "    loop_range = range(batch_size) if not test else range(1000) #To predict the safety function over 1000 points\n",
    "\n",
    "    for _ in loop_range:   \n",
    "\n",
    "        if test == False:\n",
    "            random_file = random.randint(1, total_data-1)\n",
    "        else:\n",
    "            if _ == 0:\n",
    "                random_file = random.randint(1, total_data-1)\n",
    "\n",
    "        file_route = main_dir + str(random_file) + '.mat'\n",
    "        data = scipy.io.loadmat(file_route)\n",
    "        \n",
    "        xVectors, Us, Ic = HFCN.Extended_Function_Dataset(data, mean_extension, test, _, separated_orbits)\n",
    "\n",
    "        xVectors_batch.append(xVectors)\n",
    "        Ics_batch.append(Ic)\n",
    "        Us_batch.append(Us)\n",
    "\n",
    "    xVectors_batch = np.array(xVectors_batch)\n",
    "    Ics_batch = np.array(Ics_batch)\n",
    "    Us_batch = np.array(Us_batch)\n",
    "\n",
    "    #Normalization\n",
    "    Ics_batch = Ics_batch/1000 #Normalization if necessary to keep the values between 0 and 1\n",
    "\n",
    "    return [np.array(xVectors_batch), np.array(Ics_batch)], [np.array(Us_batch)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataGenerator(Sequence):\n",
    "    def __init__(self, data, batch_size, test=False):\n",
    "        self.data = data\n",
    "        results = Parallel(n_jobs=-1)(delayed(process_data)(test=False) for _ in range(self.data))\n",
    "        self.x = np.concatenate([result[0][0] for result in results])\n",
    "        self.Ics = np.concatenate([result[0][1] for result in results])\n",
    "        self.y = np.concatenate([result[1][0] for result in results])\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.x) / float(self.batch_size)))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch_x = self.x[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        batch_Ics = self.Ics[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        batch_y = self.y[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "\n",
    "        return [np.array(batch_x), np.array(batch_Ics)], [np.array(batch_y)]\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        del self.x, self.y\n",
    "        results = Parallel(n_jobs=-1)(delayed(process_data)(test=False) for _ in range(self.data))\n",
    "        self.x = np.concatenate([result[0][0] for result in results])\n",
    "        self.Ics = np.concatenate([result[0][1] for result in results])\n",
    "        self.y = np.concatenate([result[1][0] for result in results])\n",
    "\n",
    "train_set = CustomDataGenerator(train_data, batch_size, test=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the MirroredStrategy in case there are multiple GPUs available \n",
    "# https://www.tensorflow.org/guide/distributed_training\n",
    "\n",
    "# strategy = tf.distribute.MirroredStrategy(devices=[\"/GPU:0\", \"/GPU:1\"], cross_device_ops=tf.distribute.HierarchicalCopyAllReduce())\n",
    "#with strategy.scope():\n",
    "\n",
    "input_1 = layers.Input(shape=(mean_extension, 2), name='input_1')\n",
    "input_2 = layers.Input(shape=(1,), name='input_2')\n",
    "repeated_input_2 = layers.RepeatVector(mean_extension)(input_2) # Repeat the input_2 tensor to match the shape of input_1\n",
    "\n",
    "x_input = layers.concatenate([input_1, repeated_input_2], axis=-1)\n",
    "\n",
    "# Block 1 Attention block\n",
    "\n",
    "x = layers.MultiHeadAttention(key_dim=256, num_heads=4, dropout=0.05)(x_input, x_input)\n",
    "res = layers.Add()([x, x_input])\n",
    "\n",
    "x = layers.Conv1D(filters=1024, kernel_size=1, activation=\"relu\")(res)\n",
    "x = layers.Conv1D(filters=x_input.shape[-1], kernel_size=1)(x)\n",
    "xn = layers.Add()([x, res])\n",
    "\n",
    "#Block 2 Attention block\n",
    "  \n",
    "x = layers.MultiHeadAttention(key_dim=256, num_heads=4, dropout=0.05)(xn, xn)\n",
    "res = layers.Add()([x, xn])\n",
    "\n",
    "x = layers.Conv1D(filters=1024, kernel_size=1, activation=\"relu\")(res)\n",
    "x = layers.Conv1D(filters=x_input.shape[-1], kernel_size=1)(x)\n",
    "xn = layers.Add()([x, res])\n",
    "\n",
    "#Block 3 Conv Block\n",
    "\n",
    "x = layers.Conv1D(filters=512, kernel_size=7, activation=\"relu\")(xn)\n",
    "x = layers.Dropout(0.2)(x)\n",
    "x = layers.Conv1D(filters=512, kernel_size=3, activation=\"relu\")(x)\n",
    "x = layers.Dropout(0.2)(x)\n",
    "\n",
    "#Block 4  Global Max and Average Pooling\n",
    "\n",
    "x_max = layers.GlobalMaxPooling1D()(x)\n",
    "x_avg = layers.GlobalAveragePooling1D()(x)\n",
    "x = layers.concatenate([x_max, x_avg]) \n",
    "\n",
    "# Block 5 Output Block\n",
    "\n",
    "x = layers.Dense(512, activation='relu')(x)\n",
    "x = layers.Dropout(0.2)(x)\n",
    "x = layers.Dense(256, activation='relu')(x)\n",
    "x = layers.Dropout(0.2)(x)\n",
    "x = layers.Dense(1, activation='linear', name='output')(x)\n",
    "\n",
    "# Define the model\n",
    "model = Model(inputs=[input_1, input_2], outputs=x, name=str(mean_extension) + '_input_model')\n",
    "\n",
    "loss = 'mse'\n",
    "metrics = 'mse'\n",
    "\n",
    "learning_rate = 0.00015\n",
    "final_lr = 0.000030\n",
    "\n",
    "warmup_epochs = 6\n",
    "decay_factor =  final_lr/learning_rate\n",
    "lr_schedule = LearningRateScheduler(lambda epoch, lr: HFCN.lr_warmup_scheduler(epoch, lr, learning_rate, warmup_epochs, epochs, decay_factor))\n",
    "\n",
    "optimizer = Adam(learning_rate=learning_rate)\n",
    "    \n",
    "model.compile(optimizer=optimizer, loss=HFCN.AsymmetricMSELoss(underestimation_weight=10), metrics=metrics)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The model trains with \" + str(epochs*train_data*batch_size) + \" from the \" + str(total_data) + \" total, that being \" + str(np.round(100*epochs*train_data*batch_size/total_data, 2)) + \"% of the dataset\")\n",
    "\n",
    "checkpoint_path = 'results/' + str(mean_extension) + '/'\n",
    "tensorboard_log_path = 'results/'\n",
    "\n",
    "model_checkpoint = ModelCheckpoint(checkpoint_path, monitor='loss', verbose=1, save_best_only=True) \n",
    "tensorboard = TensorBoard(log_dir = tensorboard_log_path, histogram_freq=1, write_graph=False, write_images=False) \n",
    "#early_stop = EarlyStopping('loss', patience=100, verbose=1)\n",
    "terminate = TerminateOnNaN()\n",
    "#reduce_LR_stagnate = ReduceLROnPlateau(monitor='mse', factor=0.8, patience=100, min_lr=0.0001)\n",
    "\n",
    "callbacks = [tensorboard, model_checkpoint, terminate, lr_schedule]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(train_set, \n",
    "                    epochs=epochs, \n",
    "                    batch_size=batch_size, \n",
    "                    steps_per_epoch = train_data,\n",
    "                    callbacks=callbacks)\n",
    "        \n",
    "model.save(checkpoint_path+'_modellast.h5')\n",
    "np.save(checkpoint_path+'_history1.npy',history.history) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = process_data(test=True)\n",
    "xVectors = test_set[0][0]\n",
    "ICs = test_set[0][1]\n",
    "Us = test_set[1][0]\n",
    "\n",
    "print(xVectors.shape, ICs.shape, Us.shape)\n",
    "\n",
    "test_pred = model.predict([xVectors, ICs])\n",
    "\n",
    "# Adjusting plot settings for better appearance\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# First subplot (ax1)\n",
    "\n",
    "ax1.plot(xVectors[0, :, 0], xVectors[0, :, 1], '.', markersize=4,  label='Sampled points')\n",
    "ax1.axis([-0.01, 1.01, -0.01, 1 + (np.max(xVectors) - 0.95)])\n",
    "ax1.set_title('Sample points', fontsize=12)\n",
    "ax1.set_xlabel('$\\mathrm{x_{n}}$', fontsize=10)\n",
    "ax1.set_ylabel('$\\mathrm{x_{n+1}}$', fontsize=10)\n",
    "ax1.legend(loc='best', fontsize='x-small')\n",
    "#ax1.grid(True)\n",
    "\n",
    "# Second subplot (ax2)\n",
    "ax2.plot(np.linspace(0, 1, 1000), Us, label='True safety function', linewidth=2)\n",
    "ax2.plot(np.linspace(0, 1, 1000), test_pred, label='Predicted function', color='red', linewidth=1.5)\n",
    "\n",
    "ax2.set_title('Comparison of True and Predicted Safety Functions', fontsize=12)\n",
    "ax2.set_xlabel('$\\mathrm{x_{n}}$', fontsize=10)\n",
    "ax2.set_ylabel('Safety function', fontsize=10)\n",
    "ax2.legend(loc='best', fontsize='x-small')\n",
    "\n",
    "# Third subplot (ax3)\n",
    "for i in range(len(test_pred)):\n",
    "    ax3.plot(np.linspace(0, 1, 1000), (Us - test_pred.reshape(-1)) ** 2, linewidth=1.5)\n",
    "\n",
    "ax3.set_title('Squared Difference test', fontsize=12)\n",
    "ax3.set_xlabel('$\\mathrm{x_{n}}$', fontsize=10)\n",
    "ax3.set_ylabel('mse', fontsize=10)\n",
    "ax3.legend(loc='best', fontsize='x-small')\n",
    "\n",
    "# Adjust the layout to ensure no overlap\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Optional: Adjust another figure if needed\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.plot(np.linspace(0, 1, 1000), Us, label='True safety function', linewidth=2)\n",
    "plt.plot(np.linspace(0, 1, 1000), test_pred, label='Predicted function', color='red', linewidth=1.5)\n",
    "\n",
    "plt.title('Comparison of True and Predicted Safety Functions', fontsize=12)\n",
    "plt.set_xlabel('$x_n$', fontsize=10)\n",
    "plt.set_ylabel('Safety function', fontsize=10)\n",
    "plt.legend(loc='best', fontsize='x-small')\n",
    "\n",
    "# Show the figure\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Supervised_VtoU",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
